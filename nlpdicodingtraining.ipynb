{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlpdicodingtraining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WTeLTy-oQZ2iIypQwSRe92HJ2Uu9zsa6",
      "authorship_tag": "ABX9TyNU5t68U+vEXiZYwo3bBZuh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deni1171/deni-resolusi/blob/master/nlpdicodingtraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H84u_VZhapBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29429517-52f0-4e58-c0e7-41a002fdd74c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8UvtyK_Xfql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ekstraksi file zip\n",
        "import zipfile\n",
        "import os\n",
        "local_zip = '/content/drive/My Drive/datasets/Text Emotion.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLd1J-eVao_j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "88e4cb15-ed61-4a93-8a73-eb75aab7cd66"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/tmp/Text Emotion/text_emotion.csv')\n",
        "df = df.drop(columns=['tweet_id', 'author'])\n",
        "df['content'] = df['content'].str.lower()\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>empty</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sadness</td>\n",
              "      <td>funeral ceremony...gloomy friday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>wants to hang out with friends soon!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@dannycastillo we want to trade with someone w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment                                            content\n",
              "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
              "1     sadness  layin n bed with a headache  ughhhh...waitin o...\n",
              "2     sadness                funeral ceremony...gloomy friday...\n",
              "3  enthusiasm               wants to hang out with friends soon!\n",
              "4     neutral  @dannycastillo we want to trade with someone w..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aR3XMymcFQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9f0a9498-b5e4-41fd-c6a6-0afce533d64a"
      },
      "source": [
        "# membuat one-hot-encoding dan dataset baru\n",
        "category = pd.get_dummies(df.sentiment)\n",
        "df_baru = pd.concat([df, category], axis=1)\n",
        "df_baru = df_baru.drop(columns='sentiment')\n",
        "df_baru"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>anger</th>\n",
              "      <th>boredom</th>\n",
              "      <th>empty</th>\n",
              "      <th>enthusiasm</th>\n",
              "      <th>fun</th>\n",
              "      <th>happiness</th>\n",
              "      <th>hate</th>\n",
              "      <th>love</th>\n",
              "      <th>neutral</th>\n",
              "      <th>relief</th>\n",
              "      <th>sadness</th>\n",
              "      <th>surprise</th>\n",
              "      <th>worry</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>funeral ceremony...gloomy friday...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wants to hang out with friends soon!</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@dannycastillo we want to trade with someone w...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>@johnlloydtaylor</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>happy mothers day  all my love</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>happy mother's day to all the mommies out ther...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>@niariley wassup beautiful!!! follow me!!  pee...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40000 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content  ...  worry\n",
              "0      @tiffanylue i know  i was listenin to bad habi...  ...      0\n",
              "1      layin n bed with a headache  ughhhh...waitin o...  ...      0\n",
              "2                    funeral ceremony...gloomy friday...  ...      0\n",
              "3                   wants to hang out with friends soon!  ...      0\n",
              "4      @dannycastillo we want to trade with someone w...  ...      0\n",
              "...                                                  ...  ...    ...\n",
              "39995                                   @johnlloydtaylor  ...      0\n",
              "39996                     happy mothers day  all my love  ...      0\n",
              "39997  happy mother's day to all the mommies out ther...  ...      0\n",
              "39998  @niariley wassup beautiful!!! follow me!!  pee...  ...      0\n",
              "39999  @mopedronin bullet train from tokyo    the gf ...  ...      0\n",
              "\n",
              "[40000 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCcwcq1zN26L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c0f833ed-6874-4c81-f38e-d54bef05fd22"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "inggris = stopwords.words('english')\n",
        "corpus = []\n",
        "for i in range(0, len(df_baru)):\n",
        "  content = re.sub('[^a-zA-Z]', ' ', df_baru['content'][i])\n",
        "  content = content.lower()\n",
        "  content = content.split()\n",
        "  ps = PorterStemmer()\n",
        "  content = [ps.stem(word) for word in content if not word in inggris]\n",
        "  content = ' '.join(content)\n",
        "  corpus.append(content)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1Tt0GLcdXRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataframe ke numpy array\n",
        "sentiment = df_baru['content'].values\n",
        "label = df_baru[['anger', 'boredom', 'empty', 'enthusiasm', 'fun', 'happiness', 'hate', 'love', 'neutral', 'relief', 'sadness', 'surprise', 'worry']]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGM65AFGeutY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# membagi data train dan test\n",
        "from sklearn.model_selection import train_test_split\n",
        "sentiment_train, sentiment_test, label_train, label_test = train_test_split(sentiment, label, test_size=0.2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIaa6oNfTIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ubah kata menjadi numerik\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='x')\n",
        "tokenizer.fit_on_texts(sentiment_train)\n",
        "\n",
        "sequence_train = tokenizer.texts_to_sequences(sentiment_train)\n",
        "sequence_test = tokenizer.texts_to_sequences(sentiment_test)\n",
        "\n",
        "padded_train = pad_sequences(sequence_train)\n",
        "padded_test = pad_sequences(sequence_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cuea3ZWFUApz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# memperpendek waktu pelatihan\n",
        "accuracy = 0.95\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy') >= accuracy):\n",
        "      print('mencapai akurasi yg diinginkan')\n",
        "      self.model.stop_training=True\n",
        "\n",
        "callbacks = myCallback"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgZyASLnhrA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(input_dim=9000, output_dim=16),\n",
        "                             tf.keras.layers.LSTM(64),\n",
        "                             tf.keras.layers.Dense(128, activation='relu'),\n",
        "                             tf.keras.layers.Dense(64, activation='relu'),\n",
        "                             tf.keras.layers.Dense(13, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxN2BpxujMXX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "5ded039f-6abc-4c18-ec6e-16a24db530bd"
      },
      "source": [
        "# training model\n",
        "num_epochs = 100\n",
        "history = model.fit(padded_train, label_train, epochs=num_epochs, validation_data=(padded_test, label_test), verbose=2, callbacks = [callbacks()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 2.0607 - accuracy: 0.2805 - val_loss: 1.9680 - val_accuracy: 0.3191\n",
            "Epoch 2/100\n",
            "1000/1000 - 18s - loss: 1.8724 - accuracy: 0.3589 - val_loss: 1.9214 - val_accuracy: 0.3409\n",
            "Epoch 3/100\n",
            "1000/1000 - 18s - loss: 1.7930 - accuracy: 0.3880 - val_loss: 1.9257 - val_accuracy: 0.3490\n",
            "Epoch 4/100\n",
            "1000/1000 - 18s - loss: 1.7366 - accuracy: 0.4101 - val_loss: 1.9660 - val_accuracy: 0.3483\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 1.6848 - accuracy: 0.4286 - val_loss: 1.9643 - val_accuracy: 0.3424\n",
            "Epoch 6/100\n",
            "1000/1000 - 18s - loss: 1.6320 - accuracy: 0.4486 - val_loss: 2.0118 - val_accuracy: 0.3391\n",
            "Epoch 7/100\n",
            "1000/1000 - 18s - loss: 1.5847 - accuracy: 0.4635 - val_loss: 2.0768 - val_accuracy: 0.3286\n",
            "Epoch 8/100\n",
            "1000/1000 - 19s - loss: 1.5366 - accuracy: 0.4806 - val_loss: 2.1514 - val_accuracy: 0.3385\n",
            "Epoch 9/100\n",
            "1000/1000 - 18s - loss: 1.4853 - accuracy: 0.4960 - val_loss: 2.2403 - val_accuracy: 0.3295\n",
            "Epoch 10/100\n",
            "1000/1000 - 18s - loss: 1.4369 - accuracy: 0.5120 - val_loss: 2.2636 - val_accuracy: 0.3130\n",
            "Epoch 11/100\n",
            "1000/1000 - 19s - loss: 1.3873 - accuracy: 0.5285 - val_loss: 2.4086 - val_accuracy: 0.3189\n",
            "Epoch 12/100\n",
            "1000/1000 - 18s - loss: 1.3381 - accuracy: 0.5419 - val_loss: 2.4765 - val_accuracy: 0.3066\n",
            "Epoch 13/100\n",
            "1000/1000 - 19s - loss: 1.2888 - accuracy: 0.5587 - val_loss: 2.6957 - val_accuracy: 0.3034\n",
            "Epoch 14/100\n",
            "1000/1000 - 19s - loss: 1.2377 - accuracy: 0.5741 - val_loss: 2.8330 - val_accuracy: 0.3021\n",
            "Epoch 15/100\n",
            "1000/1000 - 19s - loss: 1.1875 - accuracy: 0.5911 - val_loss: 2.9196 - val_accuracy: 0.3010\n",
            "Epoch 16/100\n",
            "1000/1000 - 20s - loss: 1.1399 - accuracy: 0.6049 - val_loss: 3.2335 - val_accuracy: 0.2973\n",
            "Epoch 17/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wyTLn4om6Z5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJS_AP0PnyEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grafik plot loss accuracy training model\n",
        "acc = model.history.history['accuracy']\n",
        "val_acc = model.history.history['val_accuracy']\n",
        "loss = model.history.history['loss']\n",
        "val_loss = model.history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label = 'Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label = 'Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy Scores')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}